[
  {
    "objectID": "posts/regression/regression.html",
    "href": "posts/regression/regression.html",
    "title": "Salary prediction via Linear Regression",
    "section": "",
    "text": "Can Salary be accurately predicted using metrics such as age and years of experience, gender and education level?\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/regression/regression.html#importing-data",
    "href": "posts/regression/regression.html#importing-data",
    "title": "Salary prediction via Linear Regression",
    "section": "Importing data",
    "text": "Importing data\n\ndataset = pd.read_csv(\"salary_data.csv\")\ndf=dataset.copy()\ndf\n\n\n\n\n\n\n\n\nAge\nGender\nEducation Level\nJob Title\nYears of Experience\nSalary\n\n\n\n\n0\n32.0\nMale\nBachelor's\nSoftware Engineer\n5.0\n90000.0\n\n\n1\n28.0\nFemale\nMaster's\nData Analyst\n3.0\n65000.0\n\n\n2\n45.0\nMale\nPhD\nSenior Manager\n15.0\n150000.0\n\n\n3\n36.0\nFemale\nBachelor's\nSales Associate\n7.0\n60000.0\n\n\n4\n52.0\nMale\nMaster's\nDirector\n20.0\n200000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n370\n35.0\nFemale\nBachelor's\nSenior Marketing Analyst\n8.0\n85000.0\n\n\n371\n43.0\nMale\nMaster's\nDirector of Operations\n19.0\n170000.0\n\n\n372\n29.0\nFemale\nBachelor's\nJunior Project Manager\n2.0\n40000.0\n\n\n373\n34.0\nMale\nBachelor's\nSenior Operations Coordinator\n7.0\n90000.0\n\n\n374\n44.0\nFemale\nPhD\nSenior Business Analyst\n15.0\n150000.0\n\n\n\n\n375 rows × 6 columns\n\n\n\n\nlabel=\"Salary\"\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nYears of Experience\nSalary\n\n\n\n\ncount\n373.000000\n373.000000\n373.000000\n\n\nmean\n37.431635\n10.030831\n100577.345845\n\n\nstd\n7.069073\n6.557007\n48240.013482\n\n\nmin\n23.000000\n0.000000\n350.000000\n\n\n25%\n31.000000\n4.000000\n55000.000000\n\n\n50%\n36.000000\n9.000000\n95000.000000\n\n\n75%\n44.000000\n15.000000\n140000.000000\n\n\nmax\n53.000000\n25.000000\n250000.000000\n\n\n\n\n\n\n\n\nfor col in df.select_dtypes(\"float\"or\"int\"):\n    print(col)\n    print(f'\\taverage with NaN: {df[col].mean()}')\n    print(f'\\taverage without NaN: {df[col].mean(skipna=True)}')\n\nAge\n    average with NaN: 37.43163538873995\n    average without NaN: 37.43163538873995\nYears of Experience\n    average with NaN: 10.03083109919571\n    average without NaN: 10.03083109919571\nSalary\n    average with NaN: 100577.34584450402\n    average without NaN: 100577.34584450402\n\n\n\nfor col in df.select_dtypes(\"object\"):\n    print(col)\n    print(f'\\tmost common value with NaN: {df[col].mode()[0]} and without NaN: {df[col].value_counts().idxmax()}')\n\nGender\n    most common value with NaN: Male and without NaN: Male\nEducation Level\n    most common value with NaN: Bachelor's and without NaN: Bachelor's\nJob Title\n    most common value with NaN: Director of Marketing and without NaN: Director of Marketing"
  },
  {
    "objectID": "posts/regression/regression.html#data-preprocessing",
    "href": "posts/regression/regression.html#data-preprocessing",
    "title": "Salary prediction via Linear Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nReplacing quantitive feature NaN with average, categorical NaN with most common value\nPrinting the whether the number of rows with NaN is 0 to make sure values were imputed properly\n\nfor col in df.columns : \n    if col in df.select_dtypes(include=['object']).columns:\n        df[col]=df[col].fillna(df[col].dropna().mode()[0])\n        print(col,df[col].isna().sum() == 0)\n        \n    else :\n        df[col]=df[col].fillna(df[col].mean(skipna=True))\n        print(col,df[col].isna().sum() == 0)\n\nAge True\nGender True\nEducation Level True\nJob Title True\nYears of Experience True\nSalary True\n\n\nCalculating value counts of each column\n\nfor col in df.select_dtypes('object'):\n    print(col,'--------&gt;')\n    print(df[col].value_counts().reset_index())\n    print()\n    print()\n\nGender --------&gt;\n   Gender  count\n0    Male    196\n1  Female    179\n\n\nEducation Level --------&gt;\n  Education Level  count\n0      Bachelor's    226\n1        Master's     98\n2             PhD     51\n\n\nJob Title --------&gt;\n                           Job Title  count\n0              Director of Marketing     14\n1             Director of Operations     11\n2            Senior Business Analyst     10\n3           Senior Marketing Analyst      9\n4           Senior Marketing Manager      9\n..                               ...    ...\n169     Business Development Manager      1\n170  Customer Service Representative      1\n171                       IT Manager      1\n172        Digital Marketing Manager      1\n173             Junior Web Developer      1\n\n[174 rows x 2 columns]\n\n\n\n\nConverting value counts to frequencies\n\nfor col in df.select_dtypes(include=['object']).columns:\n    tableau_effectifs = df[col].value_counts().reset_index()\n    tableau_effectifs.columns = [col, 'Count']\n    tableau_effectifs['Frequency (%)'] = (tableau_effectifs['Count'] / len(df)) * 100\n    print(\"Counts per\", col)\n    print(tableau_effectifs)\n    print()\n\nCounts per Gender\n   Gender  Count  Frequency (%)\n0    Male    196      52.266667\n1  Female    179      47.733333\n\nCounts per Education Level\n  Education Level  Count  Frequency (%)\n0      Bachelor's    226      60.266667\n1        Master's     98      26.133333\n2             PhD     51      13.600000\n\nCounts per Job Title\n                           Job Title  Count  Frequency (%)\n0              Director of Marketing     14       3.733333\n1             Director of Operations     11       2.933333\n2            Senior Business Analyst     10       2.666667\n3           Senior Marketing Analyst      9       2.400000\n4           Senior Marketing Manager      9       2.400000\n..                               ...    ...            ...\n169     Business Development Manager      1       0.266667\n170  Customer Service Representative      1       0.266667\n171                       IT Manager      1       0.266667\n172        Digital Marketing Manager      1       0.266667\n173             Junior Web Developer      1       0.266667\n\n[174 rows x 3 columns]\n\n\n\n\ndf_summary=df.describe()\ndf_summary\n\n\n\n\n\n\n\n\nAge\nYears of Experience\nSalary\n\n\n\n\ncount\n375.000000\n375.000000\n375.000000\n\n\nmean\n37.431635\n10.030831\n100577.345845\n\n\nstd\n7.050146\n6.539452\n48110.856588\n\n\nmin\n23.000000\n0.000000\n350.000000\n\n\n25%\n31.500000\n4.000000\n55000.000000\n\n\n50%\n36.000000\n9.000000\n95000.000000\n\n\n75%\n44.000000\n15.000000\n140000.000000\n\n\nmax\n53.000000\n25.000000\n250000.000000\n\n\n\n\n\n\n\nPlotting distribution of each quantitative feature (histogram and boxplot)\n\nfor col in df.select_dtypes(include=[\"float\",'int']).columns:\n    plt.hist(df[col], bins=10)\n    plt.xlabel(col)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Distribution of \" + col)\n    plt.show()\n    plt.boxplot(df[col])\n    plt.ylabel(col)\n    plt.title(\"Box Plot of \" + col)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting distribution of categorical features (bar chart)\n\nfor col in df.select_dtypes(\"object\").columns:\n    value_counts=df[col].value_counts()\n    if col == \"Job Title\": plt.figure(figsize=(20,20))\n    else: plt.figure(figsize=(10,10))\n    value_counts.plot.bar()\n    plt.xlabel(\"Category\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Bar chart of \" + col)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nIdentifying potential outliers by calculating 1.5*Interquartile range\n\nfor col in df.select_dtypes('float').columns:\n    q1=np.percentile(df[col],25)\n    q3=np.percentile(df[col],75)\n    iqr=q3-q1\n    inf=q1-(1.5)*iqr\n    sup=q3+(1.5)*iqr\n    outliers = df[(df[col] &lt; inf) | (df[col] &gt; sup)]\n    print(col,\"Outliers:\", outliers)\n\nAge Outliers: Empty DataFrame\nColumns: [Age, Gender, Education Level, Job Title, Years of Experience, Salary]\nIndex: []\nYears of Experience Outliers: Empty DataFrame\nColumns: [Age, Gender, Education Level, Job Title, Years of Experience, Salary]\nIndex: []\nSalary Outliers: Empty DataFrame\nColumns: [Age, Gender, Education Level, Job Title, Years of Experience, Salary]\nIndex: []\n\n\nPrinting the mean per unique value in each feature\n\nfor num_var in df.select_dtypes(\"float\"):\n    for cat_var in df.select_dtypes(\"object\"):\n        mean_per_category = df.groupby(cat_var)[num_var].mean()\n        print(f'Mean of {num_var} per {cat_var}: {mean_per_category}\\n')\n\nMean of Age per Gender: Gender\nFemale    37.581006\nMale      37.295221\nName: Age, dtype: float64\n\nMean of Age per Education Level: Education Level\nBachelor's    34.340103\nMaster's      40.765306\nPhD           44.725490\nName: Age, dtype: float64\n\nMean of Age per Job Title: Job Title\nAccount Manager                 32.0\nAccountant                      31.0\nAdministrative Assistant        37.5\nBusiness Analyst                33.5\nBusiness Development Manager    34.0\n                                ... \nUX Designer                     34.0\nUX Researcher                   27.0\nVP of Finance                   47.0\nVP of Operations                47.0\nWeb Developer                   33.0\nName: Age, Length: 174, dtype: float64\n\nMean of Years of Experience per Gender: Gender\nFemale    10.011173\nMale      10.048784\nName: Years of Experience, dtype: float64\n\nMean of Years of Experience per Education Level: Education Level\nBachelor's     6.993636\nMaster's      13.397959\nPhD           17.019608\nName: Years of Experience, dtype: float64\n\nMean of Years of Experience per Job Title: Job Title\nAccount Manager                  5.0\nAccountant                       4.0\nAdministrative Assistant         9.0\nBusiness Analyst                 6.0\nBusiness Development Manager     8.0\n                                ... \nUX Designer                      5.0\nUX Researcher                    2.0\nVP of Finance                   19.0\nVP of Operations                19.0\nWeb Developer                    6.0\nName: Years of Experience, Length: 174, dtype: float64\n\nMean of Salary per Gender: Gender\nFemale     97011.173184\nMale      103834.207611\nName: Salary, dtype: float64\n\nMean of Salary per Education Level: Education Level\nBachelor's     74984.534034\nMaster's      129795.918367\nPhD           157843.137255\nName: Salary, dtype: float64\n\nMean of Salary per Job Title: Job Title\nAccount Manager                  75000.0\nAccountant                       55000.0\nAdministrative Assistant         50000.0\nBusiness Analyst                 77500.0\nBusiness Development Manager     90000.0\n                                  ...   \nUX Designer                      80000.0\nUX Researcher                    65000.0\nVP of Finance                   200000.0\nVP of Operations                190000.0\nWeb Developer                    65000.0\nName: Salary, Length: 174, dtype: float64\n\n\n\nMapping categorical values to numerical values (Gender to 0/1, Education to numbers 1-3)\n\ndf_standarized=df\n\n\ndic_gender={\"Male\":0,\"Female\":1}\ndf_standarized['Gender']=df_standarized['Gender'].map(dic_gender)\n\n\ndic_job={\"Bachelor's\":1,\"Master's\":2,\"PhD\":3}\ndf_standarized['Education Level']=df_standarized['Education Level'].map(dic_job)\n\n\n# standardized_correlation_matrix = df_standarized.corr()\n# standardized_correlation_matrix\n\nInvestigating the correlations between each feature\n\n# plt.figure(figsize=(10,10))\n# sns.heatmap(standardized_correlation_matrix, annot=True, cmap=\"coolwarm\")\n# plt.title(\"Correlation Matrix\")\n# plt.show()\n\nLooking at trends in features across each categorical values\n\nsns.pairplot(df_standarized,hue=\"Education Level\");\nplt.title(\"Feature Trends Across Education Level\");\nplt.show();\n\n\n\n\n\nsns.pairplot(df_standarized,hue=\"Gender\");\nplt.title(\"Feature Trends Across Genders\");\nplt.show();\n\n\n\n\n\ndf_standarized_final=df_standarized.drop(\"Job Title\",axis=1)\n\n\n\nSplitting data into labels and features\n\ndef pre_processing(df):\n    X=df.drop('Salary',axis=1)\n    Y=df['Salary']\n    return X,Y\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\n\n\ntrain_set,test_set=train_test_split(df_standarized_final,test_size=0.25, random_state=0)\ntrain_set.shape\n\n(281, 5)\n\n\n\nX_train, y_train = pre_processing(train_set)\nX_test, y_test = pre_processing(test_set)"
  },
  {
    "objectID": "posts/regression/regression.html#linear-regression-model",
    "href": "posts/regression/regression.html#linear-regression-model",
    "title": "Salary prediction via Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\ndef evalution(model):\n    model.fit(X_train , y_train)\n    Ypred=model.predict(X_test)\n    plt.figure(figsize=(15,15))\n    plt.xlim(0, 210000)\n    plt.ylim(0, 210000)\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predicted\")\n    plt.title(\"Predicted Salary vs. Actual Salary\")\n    plt.scatter(y_test,Ypred,s=200)\n    plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, Ypred, 1))(np.unique(y_test)), color='red')\n    plt.legend()\n\n\nmodel=LinearRegression()\nevalution(model)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\nmodel.coef_, model.intercept_\n\n(array([ 2786.22513618, -8748.91442528, 13607.18601302,  2993.79564504]),\n -50711.1832091954)\n\n\n\ntest_x=X_test\npred=model.predict(X_test)\nprint(\"R^2:\",metrics.r2_score(y_test, pred))\nprint(\"MAE:\",metrics.mean_absolute_error(y_test, pred))\nprint(\"MSE:\",metrics.mean_squared_error(y_test, pred))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))\n\nR^2: 0.9289134708887314\nMAE: 9379.911146982055\nMSE: 158110407.0420734\nRMSE: 12574.196079355268\n\n\n\nSupport Vector Regression Model\n\nmodel=SVR(kernel=\"linear\")\nevalution(model)\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\ntest_x=X_test\npred=model.predict(X_test)\nprint(\"R^2:\",metrics.r2_score(y_test, pred))\nprint(\"MAE:\",metrics.mean_absolute_error(y_test, pred))\nprint(\"MSE:\",metrics.mean_squared_error(y_test, pred))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))\n\nR^2: 0.5686720296764955\nMAE: 25934.699187513485\nMSE: 959358148.5703752\nRMSE: 30973.5072048739"
  },
  {
    "objectID": "posts/classification/classification.html",
    "href": "posts/classification/classification.html",
    "title": "Pokemon Type Classification",
    "section": "",
    "text": "Classifying Pokemon into their Type1 based on Attack and Defense Stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n%matplotlib inline"
  },
  {
    "objectID": "posts/classification/classification.html#importing-data",
    "href": "posts/classification/classification.html#importing-data",
    "title": "Pokemon Type Classification",
    "section": "Importing Data",
    "text": "Importing Data\n\ntalk about the dataset Importing quantitative and categorical data separately. Only quantitative data will be used for classification\n\n\ndf = pd.read_csv('./pokemon.csv')\ndf2 = df.select_dtypes(include=['float64','int64'])\ndf3 = df.select_dtypes(include=['object'])\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nagainst_bug\nagainst_dark\nagainst_dragon\nagainst_electric\nagainst_fairy\nagainst_fight\nagainst_fire\nagainst_flying\nagainst_ghost\nagainst_grass\n...\nheight_m\nhp\npercentage_male\npokedex_number\nsp_attack\nsp_defense\nspeed\nweight_kg\ngeneration\nis_legendary\n\n\n\n\n0\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n0.7\n45\n88.1\n1\n65\n65\n45\n6.9\n1\n0\n\n\n1\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n1.0\n60\n88.1\n2\n80\n80\n60\n13.0\n1\n0\n\n\n2\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n2.0\n80\n88.1\n3\n122\n120\n80\n100.0\n1\n0\n\n\n3\n0.5\n1.0\n1.0\n1.0\n0.5\n1.0\n0.5\n1.0\n1.0\n0.50\n...\n0.6\n39\n88.1\n4\n60\n50\n65\n8.5\n1\n0\n\n\n4\n0.5\n1.0\n1.0\n1.0\n0.5\n1.0\n0.5\n1.0\n1.0\n0.50\n...\n1.1\n58\n88.1\n5\n80\n65\n80\n19.0\n1\n0\n\n\n\n\n5 rows × 34 columns\n\n\n\n\ndf3.head()\n\n\n\n\n\n\n\n\nabilities\ncapture_rate\nclassfication\njapanese_name\nname\ntype1\ntype2\n\n\n\n\n0\n['Overgrow', 'Chlorophyll']\n45\nSeed Pokémon\nFushigidaneフシギダネ\nBulbasaur\ngrass\npoison\n\n\n1\n['Overgrow', 'Chlorophyll']\n45\nSeed Pokémon\nFushigisouフシギソウ\nIvysaur\ngrass\npoison\n\n\n2\n['Overgrow', 'Chlorophyll']\n45\nSeed Pokémon\nFushigibanaフシギバナ\nVenusaur\ngrass\npoison\n\n\n3\n['Blaze', 'Solar Power']\n45\nLizard Pokémon\nHitokageヒトカゲ\nCharmander\nfire\nNaN\n\n\n4\n['Blaze', 'Solar Power']\n45\nFlame Pokémon\nLizardoリザード\nCharmeleon\nfire\nNaN"
  },
  {
    "objectID": "posts/classification/classification.html#data-preprocessing",
    "href": "posts/classification/classification.html#data-preprocessing",
    "title": "Pokemon Type Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nEncoding type1 labels\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\nlabels = le.fit_transform(df['type1'])\nprint(len(le.classes_))\nprint(le.classes_)\n\n18\n['bug' 'dark' 'dragon' 'electric' 'fairy' 'fighting' 'fire' 'flying'\n 'ghost' 'grass' 'ground' 'ice' 'normal' 'poison' 'psychic' 'rock' 'steel'\n 'water']\n\n\nEncoding type2 labels\n\ntype2_le = preprocessing.LabelEncoder()\ntype2 = type2_le.fit_transform(df['type2'].astype(str))\nlen(type2_le.classes_)\n\n19\n\n\nFill missing data points with the mean of that column\n\nfor i in df2:\n    if df[i].isnull().values.any():\n            df[i].fillna(df[i].mean(), inplace=True)\n\nMake sure that there are no NaN values remaining\n\ndf[list(df2)].isnull().values.any()\n\nFalse\n\n\n\ndf.loc[:, df.columns.str.contains('against')].plot(kind=\"box\", figsize=(20,10));\nplt.xticks(rotation=90);\n\n\n\n\nCreating dataset for training, combining encoded labels with imputed attack and defense values from original dataset\n\ndata = {\n    'attack': df['attack'],\n    'defense': df['defense'],\n    'sp_attack': df['sp_attack'],\n    'sp_defense': df['sp_defense'],\n    'type2': type2,\n    'type1': df['type1']\n}\ndata = pd.DataFrame(data)\ndata = df.filter(like='against').join(data)\n\nX = data.drop('type1', axis=1)\ny = data['type1']\nprint(list(X))\nX.head()\n\n['against_bug', 'against_dark', 'against_dragon', 'against_electric', 'against_fairy', 'against_fight', 'against_fire', 'against_flying', 'against_ghost', 'against_grass', 'against_ground', 'against_ice', 'against_normal', 'against_poison', 'against_psychic', 'against_rock', 'against_steel', 'against_water', 'attack', 'defense', 'sp_attack', 'sp_defense', 'type2']\n\n\n\n\n\n\n\n\n\nagainst_bug\nagainst_dark\nagainst_dragon\nagainst_electric\nagainst_fairy\nagainst_fight\nagainst_fire\nagainst_flying\nagainst_ghost\nagainst_grass\n...\nagainst_poison\nagainst_psychic\nagainst_rock\nagainst_steel\nagainst_water\nattack\ndefense\nsp_attack\nsp_defense\ntype2\n\n\n\n\n0\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n1.0\n2.0\n1.0\n1.0\n0.5\n49\n49\n65\n65\n14\n\n\n1\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n1.0\n2.0\n1.0\n1.0\n0.5\n62\n63\n80\n80\n14\n\n\n2\n1.0\n1.0\n1.0\n0.5\n0.5\n0.5\n2.0\n2.0\n1.0\n0.25\n...\n1.0\n2.0\n1.0\n1.0\n0.5\n100\n123\n122\n120\n14\n\n\n3\n0.5\n1.0\n1.0\n1.0\n0.5\n1.0\n0.5\n1.0\n1.0\n0.50\n...\n1.0\n1.0\n2.0\n0.5\n2.0\n52\n43\n60\n50\n12\n\n\n4\n0.5\n1.0\n1.0\n1.0\n0.5\n1.0\n0.5\n1.0\n1.0\n0.50\n...\n1.0\n1.0\n2.0\n0.5\n2.0\n64\n58\n80\n65\n12\n\n\n\n\n5 rows × 23 columns"
  },
  {
    "objectID": "posts/classification/classification.html#decision-tree-classifier",
    "href": "posts/classification/classification.html#decision-tree-classifier",
    "title": "Pokemon Type Classification",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\nwhat is a decision tree, how does it work, complications/shortcomings\nequations\napplications\ntalk about what I am doing in the next blocks\ncross validation\n\n\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score, KFold\nkfold = KFold(n_splits=10, shuffle=True)\n\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X,y)\n\nresult = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\n\nprint(result.mean())\n\n0.9214197530864198\n\n\nVisualizing the Tree\n\n# plotting decision tree with dilineating features -&gt; FIX LATER\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(60,36))\ntree_plot = sklearn.tree.plot_tree(clf, filled=True, rounded=True, class_names=list(y.unique()), feature_names=list(X.columns))"
  },
  {
    "objectID": "posts/classification/classification.html#logistic-regression-classifier",
    "href": "posts/classification/classification.html#logistic-regression-classifier",
    "title": "Pokemon Type Classification",
    "section": "Logistic Regression Classifier",
    "text": "Logistic Regression Classifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=48)\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\naccuracy_score(y_test, log_reg.predict(X_test))\n\nC:\\Users\\anika\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n0.9203980099502488\n\n\n\nnum_components = []\naccuracies = []\nfrom sklearn.decomposition import PCA\nfor n in range(2,21):\n    pca = PCA(n_components=n)\n    principalComponents = pca.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(principalComponents, y, test_size=0.25, random_state=48)\n    log_reg = LogisticRegression(max_iter=1000)\n    log_reg.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, log_reg.predict(X_test))\n    print( str(n) + \" Principal components produce an accuracy of: \" + str(accuracy))\n    num_components.append(n)\n    accuracies.append(accuracy*100)\n\n2 Principal components produce an accuracy of: 0.1890547263681592\n3 Principal components produce an accuracy of: 0.17412935323383086\n4 Principal components produce an accuracy of: 0.18407960199004975\n5 Principal components produce an accuracy of: 0.19900497512437812\n6 Principal components produce an accuracy of: 0.35323383084577115\n7 Principal components produce an accuracy of: 0.5024875621890548\n8 Principal components produce an accuracy of: 0.6865671641791045\n9 Principal components produce an accuracy of: 0.7611940298507462\n10 Principal components produce an accuracy of: 0.8009950248756219\n11 Principal components produce an accuracy of: 0.8507462686567164\n12 Principal components produce an accuracy of: 0.8756218905472637\n13 Principal components produce an accuracy of: 0.8955223880597015\n14 Principal components produce an accuracy of: 0.8905472636815921\n15 Principal components produce an accuracy of: 0.9054726368159204\n16 Principal components produce an accuracy of: 0.9154228855721394\n17 Principal components produce an accuracy of: 0.9104477611940298\n18 Principal components produce an accuracy of: 0.9154228855721394\n19 Principal components produce an accuracy of: 0.9203980099502488\n20 Principal components produce an accuracy of: 0.9203980099502488\n\n\nC:\\Users\\anika\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n\nplt.figure(figsize=(10,7))\nplt.plot(num_components, accuracies)\nplt.title(\"Number of Components vs. Accuracy\")\nplt.xlabel(\"# Components\")\nplt.ylabel(\"% Accuracy\")\n\nText(0, 0.5, '% Accuracy')\n\n\n\n\n\nVisualizing the classifier’s accuracy using a confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = log_reg.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\ncm = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\nplt.subplots(figsize=(20,15));\nsns.heatmap(cm, annot=True);"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anika’s ML Blogs",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "ML_Quarto_Blogs_VS",
    "section": "",
    "text": "Clustering Country Socioeconomic Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnomalous ECG Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\nDetermining whether an ECG is irregular or not using reconstruction via Autoencoders\n\n\n\n\n\n\nDec 6, 2023\n\n\nAnika Thatavarthy\n\n\n\n\n\n\n  \n\n\n\n\nPokemon Type Classification\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAnika Thatavarthy\n\n\n\n\n\n\n  \n\n\n\n\nSalary prediction via Linear Regression\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nAnika Thatavarthy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anomaly_detection/anomaly_detection.html",
    "href": "posts/anomaly_detection/anomaly_detection.html",
    "title": "Anomalous ECG Detection",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\n\nWARNING:tensorflow:From C:\\Users\\anika\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\ndf = pd.read_csv('./ecg.csv', header=None)\nraw_data = df.values\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n\n\n\n\n0\n-0.112522\n-2.827204\n-3.773897\n-4.349751\n-4.376041\n-3.474986\n-2.181408\n-1.818286\n-1.250522\n-0.477492\n...\n0.792168\n0.933541\n0.796958\n0.578621\n0.257740\n0.228077\n0.123431\n0.925286\n0.193137\n1.0\n\n\n1\n-1.100878\n-3.996840\n-4.285843\n-4.506579\n-4.022377\n-3.234368\n-1.566126\n-0.992258\n-0.754680\n0.042321\n...\n0.538356\n0.656881\n0.787490\n0.724046\n0.555784\n0.476333\n0.773820\n1.119621\n-1.436250\n1.0\n\n\n2\n-0.567088\n-2.593450\n-3.874230\n-4.584095\n-4.187449\n-3.151462\n-1.742940\n-1.490659\n-1.183580\n-0.394229\n...\n0.886073\n0.531452\n0.311377\n-0.021919\n-0.713683\n-0.532197\n0.321097\n0.904227\n-0.421797\n1.0\n\n\n3\n0.490473\n-1.914407\n-3.616364\n-4.318823\n-4.268016\n-3.881110\n-2.993280\n-1.671131\n-1.333884\n-0.965629\n...\n0.350816\n0.499111\n0.600345\n0.842069\n0.952074\n0.990133\n1.086798\n1.403011\n-0.383564\n1.0\n\n\n4\n0.800232\n-0.874252\n-2.384761\n-3.973292\n-4.338224\n-3.802422\n-2.534510\n-1.783423\n-1.594450\n-0.753199\n...\n1.148884\n0.958434\n1.059025\n1.371682\n1.277392\n0.960304\n0.971020\n1.614392\n1.421456\n1.0\n\n\n\n\n5 rows × 141 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nlabels = raw_data[:, -1]\ndata = raw_data[:, 0:-1]\n\nx_train, x_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n\n\n# scaling the data values\nmin_val = tf.reduce_min(x_train)\nmax_val = tf.reduce_max(x_train)\nx_train = (x_train - min_val) / (max_val - min_val)\nx_test = (x_test - min_val) / (max_val - min_val)\nx_train = tf.cast(x_train, tf.float32)\nx_test = tf.cast(x_test, tf.float32)\n\n\n# formatting the labels\nY_train = Y_train.astype(bool)\nY_test = Y_test.astype(bool)\n\n# segregating the normal and irregular ECG observations\nnormal_x_train = x_train[Y_train]\nirregular_x_train = x_train[~Y_train]\n\nnormal_x_test = x_test[Y_test]\nirregular_x_test = x_test[~Y_test]\n\n\n# plotting the normal and irregular ECG observations\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nax[0].plot(np.arange(140), normal_x_train[-1])\nax[0].set_title('Normal ECG')\nax[0].grid()\nax[1].plot(np.arange(140), irregular_x_train[-1])\nax[1].set_title('Irregular ECG')\nax[1].grid()\n\n\n\n\n\nfrom tensorflow.keras.models import Model\n\nclass Autoencoder(Model):\n  def __init__(self):\n    super(Autoencoder, self).__init__()\n    self.encoder = tf.keras.Sequential([\n      tf.keras.layers.Dense(140, activation='relu'),\n      tf.keras.layers.Dense(32, activation='relu'),\n      tf.keras.layers.Dense(16, activation='relu'),\n      tf.keras.layers.Dense(8, activation='relu'),\n    ])\n    self.decoder = tf.keras.Sequential([\n      tf.keras.layers.Dense(16, activation='relu'),\n      tf.keras.layers.Dense(32, activation='relu'),\n      tf.keras.layers.Dense(140, activation='sigmoid'),\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\n\nautoencoder = Autoencoder()\n\nWARNING:tensorflow:From C:\\Users\\anika\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n\n\n\n# compiling and training the model\nautoencoder.compile(optimizer='adam', loss='mae')\nautoencoder.fit(normal_x_train, normal_x_train, epochs = 20, batch_size=512, validation_data=(normal_x_test, normal_x_test))\n\nWARNING:tensorflow:From C:\\Users\\anika\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nEpoch 1/20\nWARNING:tensorflow:From C:\\Users\\anika\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n1/5 [=====&gt;........................] - ETA: 4s - loss: 0.0588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 1s 47ms/step - loss: 0.0576 - val_loss: 0.0560\nEpoch 2/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0548 - val_loss: 0.0525\nEpoch 3/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0525\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 0.0486\nEpoch 4/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0486\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0471 - val_loss: 0.0449\nEpoch 5/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0448\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0435 - val_loss: 0.0415\nEpoch 6/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0402 - val_loss: 0.0383\nEpoch 7/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0380\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 5ms/step - loss: 0.0370 - val_loss: 0.0354\nEpoch 8/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0331\nEpoch 9/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0333\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0321 - val_loss: 0.0313\nEpoch 10/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0311\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0302 - val_loss: 0.0293\nEpoch 11/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0284 - val_loss: 0.0277\nEpoch 12/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0276\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0269 - val_loss: 0.0264\nEpoch 13/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0257 - val_loss: 0.0253\nEpoch 14/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0243\nEpoch 15/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0236 - val_loss: 0.0234\nEpoch 16/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0232\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.0226\nEpoch 17/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0223\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0219 - val_loss: 0.0219\nEpoch 18/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0211\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0214 - val_loss: 0.0214\nEpoch 19/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0208 - val_loss: 0.0208\nEpoch 20/20\n1/5 [=====&gt;........................] - ETA: 0s - loss: 0.0210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/5 [==============================] - 0s 8ms/step - loss: 0.0203 - val_loss: 0.0203\n\n\n&lt;keras.src.callbacks.History at 0x242148dd990&gt;\n\n\n\nimport random\ndef plot(data, n, title):\n  enc_img = autoencoder.encoder(data)\n  dec_img = autoencoder.decoder(enc_img)\n  plt.plot(data[n], 'b')\n  plt.title(title)\n  plt.xlabel(\"Time\")\n  plt.ylabel(\"Amplitude\")\n  plt.plot(dec_img[n], 'r')\n  plt.fill_between(np.arange(140), data[n], dec_img[n], color = 'lightcoral')\n  plt.legend(labels=['Input', 'Reconstruction', 'Error'])\n  plt.show()\n\nplot(normal_x_test, random.randint(0, len(normal_x_test)), \"Normal ECG Reconstruction\")\nplot(irregular_x_test, random.randint(0, len(irregular_x_test)), \"Irregular ECG Reconstruction\")\n\n\n\n\n\n\n\n\n# calculating the training loss\nreconstructions = autoencoder.predict(normal_x_train)\ntrain_loss = tf.keras.losses.mean_squared_error(reconstructions, normal_x_train)\n\n 1/73 [..............................] - ETA: 5s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/73 [========================&gt;.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b73/73 [==============================] - 0s 890us/step\n\n\n\nthreshold = np.mean(train_loss) + np.std(train_loss)\n\n\nreconstructed_test = autoencoder.predict(x_test)\n\nlosses = tf.keras.losses.mean_squared_error(reconstructed_test, x_test)\nanomalies = tf.math.less(losses, threshold)\n\n 1/32 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nprint(str.format('Accuracy: {:.2f}', accuracy_score(Y_test, anomalies)))\nprint(str.format('Precision: {:.2f}', precision_score(Y_test, anomalies)))\nprint(str.format('Recall: {:.2f}', recall_score(Y_test, anomalies)))\n\nAccuracy: 0.94\nPrecision: 0.99\nRecall: 0.91"
  },
  {
    "objectID": "posts/clustering/clustering.html",
    "href": "posts/clustering/clustering.html",
    "title": "Clustering Country Socioeconomic Data",
    "section": "",
    "text": "Can we find patterns among countries that are considered “developed”?\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf = pd.read_csv('./countries.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\n0\nAfghanistan\n90.2\n10.0\n7.58\n44.9\n1610\n9.44\n56.2\n5.82\n553\n\n\n1\nAlbania\n16.6\n28.0\n6.55\n48.6\n9930\n4.49\n76.3\n1.65\n4090\n\n\n2\nAlgeria\n27.3\n38.4\n4.17\n31.4\n12900\n16.10\n76.5\n2.89\n4460\n\n\n3\nAngola\n119.0\n62.3\n2.85\n42.9\n5900\n22.40\n60.1\n6.16\n3530\n\n\n4\nAntigua and Barbuda\n10.3\n45.5\n6.03\n58.9\n19100\n1.44\n76.8\n2.13\n12200\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\ntotal_fer\ngdpp\n\n\n\n\ncount\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n167.000000\n\n\nmean\n38.270060\n41.108976\n6.815689\n46.890215\n17144.688623\n7.781832\n70.555689\n2.947964\n12964.155689\n\n\nstd\n40.328931\n27.412010\n2.746837\n24.209589\n19278.067698\n10.570704\n8.893172\n1.513848\n18328.704809\n\n\nmin\n2.600000\n0.109000\n1.810000\n0.065900\n609.000000\n-4.210000\n32.100000\n1.150000\n231.000000\n\n\n25%\n8.250000\n23.800000\n4.920000\n30.200000\n3355.000000\n1.810000\n65.300000\n1.795000\n1330.000000\n\n\n50%\n19.300000\n35.000000\n6.320000\n43.300000\n9960.000000\n5.390000\n73.100000\n2.410000\n4660.000000\n\n\n75%\n62.100000\n51.350000\n8.600000\n58.750000\n22800.000000\n10.750000\n76.800000\n3.880000\n14050.000000\n\n\nmax\n208.000000\n200.000000\n17.900000\n174.000000\n125000.000000\n104.000000\n82.800000\n7.490000\n105000.000000\n\n\n\n\n\n\n\n\nfeatures = ['child_mort', 'exports', 'health','imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']\nfeature_df = df[features]\n\ng = sns.pairplot(feature_df)\ng.fig.set_size_inches(12,10)\n\n\n\n\nFeatures show high correlation with each other - income + gdpp - fertility + child mortality\n\nfeature_df.drop(columns=['gdpp', 'total_fer'], axis=1, inplace=True)\nfeature_df\n\nc:\\Users\\anika\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n\n\n\n\n\n\n\n\n\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\n\n\n\n\n0\n90.2\n10.0\n7.58\n44.9\n1610\n9.44\n56.2\n\n\n1\n16.6\n28.0\n6.55\n48.6\n9930\n4.49\n76.3\n\n\n2\n27.3\n38.4\n4.17\n31.4\n12900\n16.10\n76.5\n\n\n3\n119.0\n62.3\n2.85\n42.9\n5900\n22.40\n60.1\n\n\n4\n10.3\n45.5\n6.03\n58.9\n19100\n1.44\n76.8\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n162\n29.2\n46.6\n5.25\n52.7\n2950\n2.62\n63.0\n\n\n163\n17.1\n28.5\n4.91\n17.6\n16500\n45.90\n75.4\n\n\n164\n23.3\n72.0\n6.84\n80.2\n4490\n12.10\n73.1\n\n\n165\n56.3\n30.0\n5.18\n34.4\n4480\n23.60\n67.5\n\n\n166\n83.1\n37.0\n5.89\n30.9\n3280\n14.00\n52.0\n\n\n\n\n167 rows × 7 columns\n\n\n\n\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\nscaled_features = pd.DataFrame(scaler.fit_transform(feature_df), columns=feature_df.columns)\nscaled_features\n\n\n\n\n\n\n\n\nchild_mort\nexports\nhealth\nimports\nincome\ninflation\nlife_expec\n\n\n\n\n0\n1.291532\n-1.138280\n0.279088\n-0.082455\n-0.808245\n0.157336\n-1.619092\n\n\n1\n-0.538949\n-0.479658\n-0.097016\n0.070837\n-0.375369\n-0.312347\n0.647866\n\n\n2\n-0.272833\n-0.099122\n-0.966073\n-0.641762\n-0.220844\n0.789274\n0.670423\n\n\n3\n2.007808\n0.775381\n-1.448071\n-0.165315\n-0.585043\n1.387054\n-1.179234\n\n\n4\n-0.695634\n0.160668\n-0.286894\n0.497568\n0.101732\n-0.601749\n0.704258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n162\n-0.225578\n0.200917\n-0.571711\n0.240700\n-0.738527\n-0.489784\n-0.852161\n\n\n163\n-0.526514\n-0.461363\n-0.695862\n-1.213499\n-0.033542\n3.616865\n0.546361\n\n\n164\n-0.372315\n1.130305\n0.008877\n1.380030\n-0.658404\n0.409732\n0.286958\n\n\n165\n0.448417\n-0.406478\n-0.597272\n-0.517472\n-0.658924\n1.500916\n-0.344633\n\n\n166\n1.114951\n-0.150348\n-0.338015\n-0.662477\n-0.721358\n0.590015\n-2.092785\n\n\n\n\n167 rows × 7 columns\n\n\n\nexplain inertia\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ninertia = []\nsilhouette = []\nfor num_clusters in range(2,20):\n    model = KMeans(n_clusters = num_clusters, init='k-means++', random_state=23);\n    model.fit(scaled_features);\n    inertia.append(model.inertia_)\n    silhouette.append(silhouette_score(scaled_features, model.labels_))\n\n\ninert_df = pd.DataFrame({'Num_Clusters':range(2,20), 'Inertia':inertia})\nplt.figure(figsize=(12,6))\nsns.lineplot(data=inert_df, x=\"Num_Clusters\", y=\"Inertia\", marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia');\nplt.xticks([i for i in range(2,20)]);\n\n\n\n\n\nsilh_df = pd.DataFrame({'Num_Clusters':range(2,20), 'Silhouette Score':silhouette})\nplt.figure(figsize=(12,6))\nsns.lineplot(data=silh_df, x=\"Num_Clusters\", y=\"Silhouette Score\", marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score');\nplt.xticks([i for i in range(2,20)]);\n\n\n\n\n6 clusters\n\nmodel = KMeans(n_clusters = 4, init='k-means++', random_state=23)\npreds = model.fit_predict(scaled_features)\nscaled_features['cluster'] = preds\n\n\nsns.scatterplot(x= 'imports', y='life_expec', hue='cluster', data=scaled_features)\n\n&lt;AxesSubplot:xlabel='imports', ylabel='life_expec'&gt;\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\npca= PCA(n_components=2)\nreduced_df= pd.DataFrame(pca.fit_transform(scaled_features), columns=['pca1', 'pca2'])\nreduced_df['cluster']= scaled_features['cluster']\n\n\nplt.figure(figsize=(7,5))\nax = sns.scatterplot(x='pca1', y='pca2', hue='cluster', data=reduced_df, palette='bright')\n\n\n\n\n\ncluster_0= df_final_pca.loc[df_final_pca['cluster']==0]\ncluster_0['Country'].unique()"
  }
]